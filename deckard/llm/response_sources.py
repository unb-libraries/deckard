import json
from langchain.chains import LLMChain
from logging import Logger

from deckard.core import json_dumper
from deckard.llm import fail_response
from deckard.core import extract_first_json_block

class ResponseSourceExtractor:
    """Processes response metadata to identify the contextual sources of a generated response.

    Args:
        chain (LLMChain): The LLM chain to infer against.
        logger (Logger): The logger
    """
    def __init__(self, chain: LLMChain, logger: Logger) -> None:
        self.chain = chain
        self.logger = logger

    def get_sources(self, query, response, chunks_used, top_n=8) -> tuple:
        """ Determines the sources of a generated response.

        This is really fragile as it depends on the response metadata being in a specific format. This should eventually
        be replaced with more static-oriented arguments.

        Args:
            query (str): The query
            response (str): The response
            chunks_used (list(dict)): the chunks used to assemble to context.

        Returns:
            tuple (str, bool, bool): The response/summarization was run/at least one of the queries was answered.
        """
        sources_used = []

        for chunk in chunks_used:
            if chunk['metadata']['source_type'] == 'webpage' and chunk['metadata']['source'] is not None and chunk['metadata']['source'] != '':
                print (chunk['metadata']['source'])
                sources_used.append( {
                    "chunk": chunk['chunk'],
                    "source": chunk['metadata']['source']
                    }
                )
                if len(sources_used) >= top_n:
                    break
        self.logger.info("Determining sources of response")
        chain_response = self.chain.invoke(
            {
                "llm_response": response,
                "retrieved_chunks": json_dumper(sources_used)
            }
        )

        self.response = chain_response
        try:
            json_response = extract_first_json_block(self.response, "Sources", self.logger)
            if json_response is None:
                reason = "Error parsing sources: no JSON found in extracted text"
                self.logger.error(reason)
                return False, reason, []
            if 'source_urls' in json_response:
                return json_response['source_urls'], "Success", json_response
            else:
                reason = "Error parsing sources: 'source_urls' not found in eval"
                self.logger.error(reason)
                return False, reason, []
        except Exception as e:
            reason = f"Error parsing sources: {e}"
            self.logger.error(reason)
            return False, reason, []

def get_sources_prompt() -> str:
    """Returns the prompt that instructs the LLM to identify the sources of a generated response.

    Returns:
        str: The prompt.
    """
    return """<|start_header_id|>system<|end_header_id|> 
You are a highly precise AI agent that supports retrieval-augmented generation (RAG) workflows. Your task is to analyze a set of retrieved context chunks and determine which source URLs directly contributed factual content or specific language to the final response.

- Only include URLs for chunks that clearly contributed **verifiable facts, specific phrasing, or unique information** that appears in the response.
- Do **not** include chunks that are merely topically related or share general themes.
- Err on the side of **exclusion**. If there is any doubt, assume the chunk did **not** contribute.
- Your output must be in **strict JSON** format. No comments, no additional explanations.

<|eot_id|>

<|start_header_id|>user<|end_header_id|> 
Here is a response generated by an LLM along with the retrieved context chunks. Identify **only the source URLs of the chunks that contributed directly** to the response.

#### Response:
```
{llm_response}
```

#### Retrieved Chunks:
```json
{retrieved_chunks}  // Each chunk includes metadata with a "source_url" field.
```

#### Instructions:
1. Identify specific **facts, phrases, or claims** in the response.
2. Match them **exactly or near-exactly** to language in the retrieved chunks.
3. Only include the `source_url` of chunks that **clearly provided content** used in the response.
4. Do **not** infer or guess based on topic overlap alone.
5. If no chunk content was used in the response, return an empty list.

## Output:
```json
{{
  "source_urls": ["URL1", "URL2"]
}}
```

Respond **only** with the JSON object. No additional text.

<|eot_id|><|start_header_id|>assistant<|end_header_id|>
"""
